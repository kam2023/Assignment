{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ce66e2-4088-4de5-ab65-96e7febcf3ce",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "+ Polynomial functions and kernel functions are both used in machine learning algorithms, particularly in support vector machines (SVMs).\n",
    "\n",
    "+ In SVMs, a kernel function is used to map data from the original input space into a higher-dimensional feature space, where linear separation of classes is possible. Polynomial functions are one type of kernel function that can be used for this purpose.\n",
    "\n",
    "+ Polynomial kernel functions are defined as:\n",
    "\n",
    "K(x, y) = (x * y + c)^d\n",
    "\n",
    "+ where x and y are input vectors, c is a constant, and d is the degree of the polynomial. This kernel function corresponds to a polynomial of degree d in the original input space.\n",
    "\n",
    "+ The advantage of using polynomial kernel functions is that they can capture nonlinear relationships between input features. SVMs using polynomial kernel functions are particularly useful for classification tasks where there are complex decision boundaries.\n",
    "\n",
    "+ In summary, polynomial functions can be used as kernel functions in machine learning algorithms, particularly in SVMs, to map data into higher-dimensional feature spaces where linear separation of classes is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcfc17-aa47-46bc-9c4c-0a78dbea90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "## Implementing an SVM with a polynomial kernel in Python using Scikit-learn is relatively straightforward. \n",
    "\n",
    "## First, you'll need to import the necessary libraries:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644e258-f78b-45c6-aeac-49ddd4ec3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you can load the dataset you want to use. For example, here we'll use the iris dataset:\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675435a-0a34-4f7b-897d-45ffe192f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, you'll need to split the data into training and testing sets:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a032d2-8478-40b8-9691-970c4ae6a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you can create an SVM with a polynomial kernel using the 'SVC' class:\n",
    "\n",
    "model = SVC(kernel='poly', degree=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454859f-cdfb-4228-977d-accc2eb618b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we've set the kernel to 'poly' to use a polynomial kernel, and we've set the degree to 3.\n",
    "\n",
    "## You can then fit the model to the training data:\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490e6cc-32bf-4b4d-98b3-d632a83d3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    " And finally, you can make predictions on the test data and calculate the accuracy:\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# This will output the accuracy of the model on the test data. You can experiment with different values \n",
    "# of the degree parameter to see how it affects the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377190-14b6-48a2-9dd0-9eadb11839aa",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "+ In Support Vector Regression (SVR), epsilon is the parameter that controls the width of the margin around the predicted function where no penalty is incurred. Increasing the value of epsilon will increase the width of the margin and allow more data points to be within the margin.\n",
    "\n",
    "+ As the width of the margin increases, the number of support vectors required to define the boundary between the predicted function and the training data will generally decrease. This is because more data points will be allowed to fall within the margin without penalty, reducing the need for support vectors to define the boundary.\n",
    "\n",
    "+ However, it's important to note that the relationship between epsilon and the number of support vectors is not always straightforward and can depend on the specific data and the other parameters used in the SVR algorithm. In some cases, increasing epsilon may result in more complex decision boundaries and thus more support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e6f09-ca48-40b0-b954-ead070394d2e",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "# affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "# and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "## Support Vector Regression (SVR) is a powerful algorithm that can be fine-tuned by adjusting various parameters. Each of the parameters - kernel function, C parameter, epsilon parameter, and gamma parameter - can affect the performance of the algorithm in different ways.\n",
    "\n",
    "1. Kernel function:\n",
    "\n",
    "The kernel function determines the shape of the decision boundary and how the input data is mapped into a higher-dimensional feature space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "+ Linear kernel: It is the simplest kernel function, which works well for linearly separable data. This kernel is a good choice when there is no need to project the data into a higher dimensional space.\n",
    "+ Polynomial kernel: It maps the data into a higher dimensional space, which makes it suitable for non-linearly separable data. Increasing the degree of the polynomial will lead to a more complex decision boundary.\n",
    "+ RBF kernel: It is a popular kernel function that is often used in SVR. It maps the data into an infinite-dimensional space and is useful for capturing complex patterns in the data.\n",
    "+ Sigmoid kernel: It is often used in text classification tasks where the input data is represented as a sparse matrix of word occurrences.\n",
    "\n",
    "2. C parameter:\n",
    "\n",
    "The C parameter controls the trade-off between maximizing the margin and minimizing the error. A smaller value of C will result in a wider margin, but more training errors will be allowed. A larger value of C will result in a smaller margin, but fewer training errors will be allowed.\n",
    "\n",
    "+ A smaller value of C should be used when the data is noisy or when the decision boundary is expected to be smooth.\n",
    "+ A larger value of C should be used when the data is less noisy or when a complex decision boundary is expected.\n",
    "\n",
    "3. Epsilon parameter:\n",
    "\n",
    "The epsilon parameter controls the width of the margin around the predicted function where no penalty is incurred. Increasing the value of epsilon will increase the width of the margin and allow more data points to be within the margin.\n",
    "\n",
    "+ A smaller value of epsilon should be used when the data is noisy or when a smooth function is expected.\n",
    "+ A larger value of epsilon should be used when the data is less noisy or when a more flexible function is expected.\n",
    "\n",
    "4. Gamma parameter:\n",
    "\n",
    "The gamma parameter controls the shape of the decision boundary for the RBF kernel. A smaller value of gamma will result in a smoother decision boundary, while a larger value of gamma will result in a more complex decision boundary that can potentially overfit the data.\n",
    "\n",
    "+ A smaller value of gamma should be used when the data is less complex or when the decision boundary is expected to be smooth.\n",
    "+ A larger value of gamma should be used when the data is more complex or when the decision boundary is expected to be more complex.\n",
    "\n",
    "+ In summary, the choice of parameters in SVR can have a significant impact on its performance. It is important to carefully tune these parameters to achieve the best results for the specific data and task at hand. A good way to do this is by using a validation set to evaluate the performance of the model for different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc790106-4d2d-43ce-8fbc-0cd9ccaf5920",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVR\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m boston \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_boston\u001b[49m()\n\u001b[1;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# Q5. Assignment:\n",
    "\n",
    "# Import the necessary libraries and load the dataset\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load dataset\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Initialize and fit the SVR model\n",
    "model = SVR(kernel='linear', C=1.0, epsilon=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the output and print the results\n",
    "y_pred = model.predict(X)\n",
    "print(\"Predicted output:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0c9a86-4d15-4ab5-b161-f74b2c9044ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVR\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m boston \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_boston\u001b[49m()\n\u001b[1;32m     12\u001b[0m X \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load dataset\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and fit the SVR model\n",
    "model = SVR(kernel='linear', C=1.0, epsilon=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the output on the test set and print the results\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predicted output:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82a4c9-5609-4130-8beb-d9c71796eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using any technique of your choice (e.g. scaling, normalisation)\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load dataset\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data using scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the SVR model\n",
    "model = SVR(kernel='linear', C=1.0, epsilon=0.1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the output on the test set and print the results\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"Predicted output:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fd8db-ed54-4ee8-9b19-10739672214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966f18b-2136-4456-9c8b-cd89d118e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained classifier to predict the labels of the testing data\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Predicted labels: \", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0649af7-d24e-4a42-bca2-f38175d04a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scores)\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea02117-3b0f-4246-ae82-8d30fd658ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Create an instance of the GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1474b1f-d330-4dec-9b1d-b65d1289fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tuned classifier on the entire dataset\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Create an instance of the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the entire dataset\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Create an instance of the SVC classifier with the best parameters found by GridSearchCV\n",
    "best_svc = SVC(C=grid_search.best_params_['C'], kernel=grid_search.best_params_['kernel'], gamma=grid_search.best_params_['gamma'])\n",
    "\n",
    "# Train the classifier on the entire dataset\n",
    "best_svc.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824ce57-dd28-453a-87d0-6d7a95c4aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier to a file for future use.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pickle\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "filename = 'trained_classifier.sav'\n",
    "pickle.dump(best_svc, open(filename, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
