{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b4316-1dc2-4721-abe2-306be62cab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a random forest classifier to predict the risk of heart disease based on a dataset of patient\n",
    "# information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type,\n",
    "# resting blood pressure, serum cholesterol, and maximum heart rate achieved.\n",
    "# Dataset link: https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?\n",
    "\n",
    "# Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "# numerical features if necessary.\n",
    "\n",
    "# that uses scikit-learn library to build a random forest classifier to predict the risk of heart disease based on the dataset you provided:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92c17f-34a6-4a3c-93c1-a4abc010c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "# Python code that splits the dataset into a training set (70%) and a test set (30%) using scikit-learn's 'train_test_split' function:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "score = rfc.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b858140-83c7-4e4e-940f-0eb5fe9446b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "# tree. Use the default values for other hyperparameters.\n",
    "\n",
    "# Python code that trains a random forest classifier on the training set with 100 trees and a maximum depth of 10 for each tree:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "score = rfc.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6c172-4e81-4416-8428-17ba11937f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "# Python code that trains a random forest classifier on the training set with 100 trees and a maximum depth of 10 for each tree, \n",
    "# and then evaluates its performance on the test set using accuracy, precision, recall, and F1 score\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f72ce5-f24e-4f29-b60c-59da3fe39930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c51e0-0076-477b-adac-0bb7b990783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "# disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "#  that trains a random forest classifier on the dataset, computes the feature importances, and visualizes them using a bar chart\n",
    "# to identify the top 5 most important features:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Train the random forest classifier\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rfc.fit(X, y)\n",
    "\n",
    "# Compute feature importances\n",
    "importances = rfc.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = importances.argsort()[::-1]\n",
    "\n",
    "# Print the top 5 most important features\n",
    "print(\"Top 5 most important features:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]}\")\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c317bdf-dc0d-4366-8b6e-921467a1691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute the feature importances and visualize them using a bar chart:\n",
    "\n",
    "importances = rfc.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = importances.argsort()[::-1]\n",
    "\n",
    "# Print the top 5 most important features\n",
    "print(\"Top 5 most important features:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]}\")\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure()\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a3683-2b25-49d7-9121-a771721c7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "# different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "# leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Load the data\n",
    "url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = url.split(\"/\")[-2]\n",
    "dwn_url = 'https://drive.google.com/uc?id=' + file_id\n",
    "df = pd.read_csv(dwn_url)\n",
    "\n",
    "# Preprocess the data\n",
    "# ...\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the random forest classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133baf4-54f6-4019-8da4-54d5d720bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "# metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocess the data\n",
    "# ...\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "# ...\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform a grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and corresponding performance metrics\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision score: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall score: \", recall_score(y_test, y_pred))\n",
    "print(\"F1 score: \", f1_score(y_test, y_pred))\n",
    "\n",
    "# Compare with default model\n",
    "rf_default = RandomForestClassifier(random_state=42)\n",
    "rf_default.fit(X_train, y_train)\n",
    "y_pred_default = rf_default.predict(X_test)\n",
    "print(\"Default model accuracy score: \", accuracy_score(y_test, y_pred_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c092cbc-379d-4b28-8d69-87884c0eb846",
   "metadata": {},
   "source": [
    "# Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "# decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "# limitations of the model for predicting heart disease risk.\n",
    "\n",
    "+ Interpreting the decision boundaries of a random forest classifier can be challenging since it involves analysing the combined effect of multiple decision trees. One way to visualise the decision boundaries is to use a scatter plot of two of the most important features and overlay the decision boundaries of the model.\n",
    "\n",
    "+ To do this, we can first train a random forest classifier with the best set of hyperparameters found in the previous step. We can then extract the two most important features from the feature importance scores and create a scatter plot of these two features for the test set. We can use the predict_proba method of the trained model to generate a probability estimate for each point on the scatter plot. We can then use a contour plot to overlay the decision boundaries of the model on the scatter plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53da0ab-a238-4c9d-9df5-cb5d61bfae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  code snippet to implement this\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# load the dataset\n",
    "url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = url.split('/')[-2]\n",
    "dwn_url = 'https://drive.google.com/uc?id=' + file_id\n",
    "df = pd.read_csv(dwn_url)\n",
    "\n",
    "# split the data into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# preprocess the data\n",
    "X = pd.get_dummies(X, columns=['cp', 'thal', 'slope'])\n",
    "X = X.fillna(X.mean())\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# train a random forest classifier with the best hyperparameters found by grid search\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# select the two most important features\n",
    "importances = rfc.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feat1 = indices[0] # index of the most important feature\n",
    "feat2 = indices[1] # index of the second most important feature\n",
    "\n",
    "# create a scatter plot of the two features\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, feat1], X_test[:, feat2], c=y_test, cmap='bwr')\n",
    "plt.xlabel(df.columns[feat1])\n",
    "plt.ylabel(df.columns[feat2])\n",
    "plt.title(\"Scatter Plot of Two Most Important Features\")\n",
    "\n",
    "# generate a probability estimate for each point on the scatter plot\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "Z = rfc.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# overlay the decision boundaries of the model on the scatter plot\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='k')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
