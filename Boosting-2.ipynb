{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6026c6-58e0-434a-a324-c209a7fdaa76",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "##  Gradient Boosting Regression, also known as Gradient Boosted Regression Trees (GBRT) or Gradient Boosting Machines (GBM), is a popular machine learning algorithm for regression tasks. It is an ensemble method that combines multiple weak predictive models (typically decision trees) to create a stronger and more accurate predictive model.\n",
    "\n",
    "+ The basic idea behind gradient boosting regression is to iteratively train a sequence of weak models, where each subsequent model tries to correct the mistakes made by the previous models. In other words, each model is built to predict the residual errors (the differences between the actual and predicted values) of the previous models.\n",
    "\n",
    "+ Here's a high-level overview of the gradient boosting regression process:\n",
    "\n",
    "1. Initially, a simple model, such as a decision tree with a shallow depth, is trained on the data.\n",
    "2. The model's predictions are compared to the actual target values, and the differences (residual errors) are calculated.\n",
    "3. A new weak model is trained to predict these residual errors.\n",
    "4. The predictions of this new model are added to the previous model's predictions to obtain an updated set of predictions.\n",
    "5. Steps 2-4 are repeated iteratively, with each new model trying to minimize the errors made by the previous models.\n",
    "6. The final prediction is obtained by combining the predictions of all the models in the ensemble.\n",
    "\n",
    "+ Gradient boosting regression uses a gradient descent optimization algorithm to find the best parameters for each weak model at each iteration. The learning process involves minimizing a loss function, such as mean squared error (MSE) or mean absolute error (MAE), by adjusting the parameters of the weak models.\n",
    "\n",
    "+ Some advantages of gradient boosting regression include its ability to handle complex relationships between features and targets, robustness to outliers, and the flexibility to work with different loss functions. However, it can be prone to overfitting if not properly regularized and tuned.\n",
    "\n",
    "+ Overall, gradient boosting regression is a powerful and widely used algorithm for regression tasks, capable of producing accurate predictions in a variety of domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e527d7-5369-486f-97f6-1ea4acfe253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "# simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "# performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean value\n",
    "        y_pred = np.mean(y) * np.ones_like(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residual errors\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Fit a new weak model to the residuals\n",
    "            model = DecisionTreeRegressor(max_depth=1)\n",
    "            model.fit(X, residuals)\n",
    "\n",
    "            # Update the predictions with the weak model's predictions\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "\n",
    "            # Add the weak model and its weight to the ensemble\n",
    "            self.models.append(model)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by combining the predictions of all weak models\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            y_pred += weight * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "# Assuming X and y are your input features and target values, respectively\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43bfa7-058c-4e36-91c9-3bfaf423e674",
   "metadata": {},
   "source": [
    "+ In the above code, we use the 'GradientBoostingRegressor' class to implement the gradient boosting algorithm. The 'fit' method trains the model on the input features 'X' and target values 'y'. The algorithm initializes with the mean value of 'y' and iteratively fits weak models (decision trees with a maximum depth of 1) to the residual errors, updating the predictions at each step. The 'predict' method makes predictions on new data points by combining the predictions of all the weak models in the ensemble.\n",
    "\n",
    "+ After training the model, we evaluate its performance using mean squared error (MSE) and R-squared metrics. The MSE measures the average squared difference between the predicted and actual values, while R-squared represents the proportion of the variance in the target variable that is predictable from the input features.\n",
    "\n",
    "+ Please note that this is a simplified implementation for educational purposes. In practice, you may want to incorporate additional features, hyperparameter tuning, regularization techniques, and consider using established libraries like scikit-learn for more efficient and optimized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee6e05-6e8e-467b-bb26-93f62d5b49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "# optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Assuming X and y are your input features and target values, respectively\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Define the parameter distributions for random search\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create the scoring functions\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring={'MSE': mse_scorer, 'R-squared': r2_scorer},\n",
    "    refit='MSE',\n",
    "    cv=3,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and results\n",
    "print(\"Grid Search Results:\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE:\", -grid_search.best_score_)\n",
    "print(\"Best R-squared:\", grid_search.cv_results_['mean_test_R-squared'][grid_search.best_index_])\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_distributions=param_distributions,\n",
    "    scoring={'MSE': mse_scorer, 'R-squared': r2_scorer},\n",
    "    refit='MSE',\n",
    "    cv=3,\n",
    "    return_train_score=True,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and results\n",
    "print(\"\\nRandom Search Results:\")\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best MSE:\", -random_search.best_score_)\n",
    "print(\"Best R-squared:\", random_search.cv_results_['mean_test_R-squared'][random_search.best_index_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4918c-5024-43fc-b3b3-57f13d3ea416",
   "metadata": {},
   "source": [
    "+ In the above code, we use the 'GridSearchCV' and 'RandomizedSearchCV' classes from scikit-learn to perform grid search and random search, respectively. We define the parameter grid or distribution, the scoring functions (MSE and R-squared), and specify the number of cross-validation folds ('cv' parameter).\n",
    "\n",
    "+ The grid search is performed using 'GridSearchCV', where the algorithm systematically evaluates all possible combinations of the specified hyperparameters. The best hyperparameters are determined based on the MSE score ('refit='MSE'), but we also compute and print the R-squared score for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca3a2b-5421-41aa-926d-85a87c0c9f46",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "+ In the context of gradient boosting, a weak learner refers to a simple and relatively low-complexity model that is used as a building block in the ensemble. Typically, decision trees with shallow depths (e.g., one or two levels) are commonly used as weak learners in gradient boosting algorithms.\n",
    "\n",
    "+ The idea behind using weak learners in gradient boosting is that by sequentially adding them to the ensemble, each subsequent weak learner focuses on capturing the patterns and relationships that were not well captured by the previous weak learners. In this way, the ensemble gradually improves its predictive power by reducing the errors or residuals made by the previous models.\n",
    "\n",
    "+ Weak learners are often referred to as \"weak\" because they have limitations in their individual predictive capabilities. A single weak learner may have a high bias and relatively low accuracy when making predictions on its own. However, when combined in an ensemble using boosting techniques like gradient boosting, these weak learners can work together to create a stronger and more accurate model.\n",
    "\n",
    "+ In gradient boosting, the weak learners are typically constructed as decision trees with shallow depths to ensure simplicity and prevent overfitting. Shallow decision trees are less prone to overfitting as they have fewer degrees of freedom and are less likely to capture noise or irrelevant patterns in the data. The simplicity of weak learners also allows for faster training and inference times compared to more complex models.\n",
    "\n",
    "+ By iteratively adding weak learners to the ensemble and optimizing their contributions using gradient descent, the gradient boosting algorithm can effectively build a powerful predictive model by leveraging the strengths of multiple weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0cc53-f387-4e36-aa9e-adcf62de5b13",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "## The intuition behind the Gradient Boosting algorithm lies in the idea of building a strong predictive model by combining multiple weak models in a sequential manner. The algorithm seeks to iteratively correct the mistakes made by the previous models, gradually improving the overall predictive power of the ensemble.\n",
    "\n",
    "+ Here's a step-by-step intuition behind the Gradient Boosting algorithm:\n",
    "\n",
    "1. Initially, a simple model (weak learner) is trained on the data, typically with equal weights assigned to each sample.\n",
    "\n",
    "2. The predictions of this initial model are compared to the actual target values, and the differences (residual errors) between the predictions and the actual values are calculated.\n",
    "\n",
    "3. A new weak model is then trained to predict these residual errors. The goal is to find a model that can capture the patterns and relationships in the data that were not adequately captured by the initial model.\n",
    "\n",
    "4. The predictions of this new model, multiplied by a small learning rate (which determines the contribution of each model), are added to the previous model's predictions, yielding an updated set of predictions.\n",
    "\n",
    "5. Steps 2-4 are repeated iteratively, with each new model focusing on the remaining errors or residuals left by the previous models. The algorithm places more emphasis on the samples that were poorly predicted by the previous models.\n",
    "\n",
    "6. Finally, the ensemble model is constructed by combining the predictions of all the weak models, weighted by their respective learning rates. This ensemble model represents the boosted or aggregated prediction of the individual weak models.\n",
    "\n",
    "+ The intuition behind gradient boosting comes from the gradient descent optimization algorithm. In each iteration, the algorithm computes the negative gradient (the direction of steepest descent) of the loss function with respect to the predicted values. The new weak model is then trained to approximate this negative gradient, effectively minimizing the loss function and reducing the errors made by the ensemble.\n",
    "\n",
    "+ By combining multiple weak models and optimizing their contributions based on the gradient of the loss function, gradient boosting can create a powerful predictive model that is capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af196c-289b-4d7e-a99a-1f9f400107d2",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "## The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. Each weak learner is trained to minimize the errors made by the previous models in the ensemble. Here's how the algorithm builds the ensemble:\n",
    "\n",
    "1. Initialize the ensemble: Initially, the ensemble is empty, and the predicted values are set to a default value, such as the mean of the target variable.\n",
    "\n",
    "2. Compute the residuals: The difference between the actual target values and the current predictions is computed. These differences, known as the residuals or pseudo-residuals, represent the errors made by the current ensemble.\n",
    "\n",
    "3. Train a weak learner: A weak learner, often a decision tree with a small depth, is trained to predict the residuals. The weak learner is fit to the training data, where the features remain the same, but the target values are replaced with the computed residuals.\n",
    "\n",
    "4. Update the ensemble: The predictions of the weak learner are added to the current predictions of the ensemble, with a scaling factor called the learning rate. The learning rate determines the contribution of each weak learner to the ensemble. The learning rate is typically a small value (e.g., 0.01 or 0.1) to control the overall impact of each weak learner.\n",
    "\n",
    "5. Repeat steps 2-4: The process is repeated iteratively, with each iteration focusing on reducing the errors made by the current ensemble. At each step, new weak learners are trained to predict the negative gradients of the loss function with respect to the current predictions. The ensemble's predictions are updated, and the residuals are recomputed based on the new predictions.\n",
    "\n",
    "6. Termination: The iterative process continues until a predefined stopping criterion is met. This criterion can be a maximum number of iterations, reaching a specific level of performance, or other convergence conditions.\n",
    "\n",
    "7. Final ensemble: The final ensemble is formed by combining the predictions of all the weak learners in the ensemble, typically by summing them up.\n",
    "\n",
    "+ The ensemble of weak learners in Gradient Boosting works together to improve the overall predictive power. Each weak learner is specialized in capturing a specific pattern or relationship in the data, complementing the weaknesses of the previous models. By sequentially updating the predictions based on the weak learners' contributions, the ensemble becomes increasingly accurate and capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530e0c5-868b-4fd8-b848-2a67371681b9",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "## Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the underlying optimization and regression principles. Here are the key steps involved:\n",
    "\n",
    "1. Define the loss function: Start by defining a suitable loss function that quantifies the discrepancy between the predicted values and the true target values. For regression tasks, the commonly used loss functions include mean squared error (MSE) or mean absolute error (MAE).\n",
    "\n",
    "2. Initialize the ensemble: Set an initial prediction for the ensemble, often as a constant value like the mean of the target variable. This initializes the iterative process.\n",
    "\n",
    "3. Compute the negative gradient: Calculate the negative gradient of the loss function with respect to the current predictions. The negative gradient provides the direction of steepest descent in the loss function space.\n",
    "\n",
    "4. Train a weak learner: Fit a weak learner, typically a decision tree with shallow depth, to the negative gradient. The weak learner is trained to approximate the negative gradient, which captures the direction in which the predictions need to be adjusted to minimize the loss function.\n",
    "\n",
    "5. Update the ensemble: Add the predictions of the weak learner, scaled by a learning rate, to the current predictions of the ensemble. The learning rate controls the contribution of each weak learner and is usually a small value (<1).\n",
    "\n",
    "6. Repeat steps 3-5: Iterate the process by recomputing the negative gradient based on the residuals (the difference between the current predictions and the true target values). Train a new weak learner to approximate the negative gradient, update the ensemble predictions, and adjust the residuals.\n",
    "\n",
    "7. Termination: Stop the iterations based on a predefined stopping criterion, such as reaching a maximum number of iterations or observing negligible improvement in the loss function.\n",
    "\n",
    "8. Final ensemble: The final ensemble is formed by combining the predictions of all the weak learners in the ensemble. Typically, the predictions are summed up to yield the final prediction.\n",
    "\n",
    "+ Mathematically, Gradient Boosting minimizes the loss function by iteratively adding weak learners that approximate the negative gradient of the loss function. The ensemble of weak learners collectively improves the model's predictions by iteratively reducing the residuals or errors made by the previous models.\n",
    "\n",
    "+ During the optimization process, the algorithm leverages the concept of gradient descent, where the negative gradient guides the direction of updates to move towards the optimal point in the loss function space. The learning rate controls the step size of these updates, ensuring a gradual convergence towards the optimal solution.\n",
    "\n",
    "+ By iteratively updating the ensemble based on weak learners and minimizing the loss function, Gradient Boosting constructs a powerful predictive model that can capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d9b1f-411b-454d-96da-7a7afeff2da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
