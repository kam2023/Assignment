{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8816c05-cae9-4ac2-80b4-57dfa5d66796",
   "metadata": {},
   "source": [
    "# Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example.\n",
    "\n",
    "+ Probability Mass Function (PMF) and Probability Density Function (PDF) are both mathematical tools used in probability theory to describe the probability distribution of a random variable.\n",
    "\n",
    "+ A Probability Mass Function (PMF) is a function that assigns a probability to each possible value of a discrete random variable. The PMF gives the probability of each outcome in the sample space of the random variable. The sum of all probabilities must be equal to 1. The PMF is defined as:\n",
    "\n",
    "P(X = x) = probability of X taking the value x\n",
    "\n",
    "+ where X is the random variable and x is one of the possible values that X can take.\n",
    "\n",
    "+ For example, consider a random variable X that represents the number of heads obtained when flipping a fair coin twice. The possible values of X are 0, 1, or 2. The PMF of X is given by:\n",
    "\n",
    "P(X=0) = 1/4\n",
    "P(X=1) = 1/2\n",
    "P(X=2) = 1/4\n",
    "\n",
    "+ This PMF tells us that the probability of getting no heads is 1/4, the probability of getting one head is 1/2, and the probability of getting two heads is 1/4.\n",
    "\n",
    "+ A Probability Density Function (PDF) is a function that describes the relative likelihood for a continuous random variable to take on a given value. Unlike the PMF, which is defined for discrete random variables, the PDF is defined for continuous random variables. The area under the curve of a PDF between any two values represents the probability of the random variable taking on a value within that interval.\n",
    "\n",
    "+ For example, consider a continuous random variable X that represents the height of individuals in a population. The PDF of X might be a normal distribution with a mean of 170 cm and a standard deviation of 10 cm. The PDF is given by:\n",
    "\n",
    "f(x) = (1 / (sqrt(2 * pi) * 10)) * exp(-(x - 170)^2 / (2 * 10^2))\n",
    "\n",
    "+ This PDF tells us the relative likelihood of an individual having a height of x. The area under the curve between any two values of x represents the probability of an individual having a height within that range. For example, the probability of an individual having a height between 160 cm and 180 cm is the area under the curve between those two values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ee28a-d723-4d7d-ba59-c7ea3aac21fe",
   "metadata": {},
   "source": [
    "# Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n",
    "\n",
    "+ Cumulative Density Function (CDF) is a mathematical function that describes the probability that a random variable takes on a value less than or equal to a given value. It is defined for both discrete and continuous random variables.\n",
    "\n",
    "+ For a discrete random variable X, the CDF is defined as:\n",
    "\n",
    "F(x) = P(X <= x) = ∑ P(X = k) for all k <= x\n",
    "\n",
    "+ where X is the random variable, x is a possible value of X, and P(X <= x) is the probability that X is less than or equal to x.\n",
    "\n",
    "+ For a continuous random variable X, the CDF is defined as:\n",
    "\n",
    "F(x) = P(X <= x) = ∫ f(t) dt from -∞ to x\n",
    "\n",
    "+ where f(x) is the probability density function (PDF) of X, and the integral is taken over all values of t from negative infinity to x.\n",
    "\n",
    "+ For example, consider a discrete random variable X that represents the number of defective items in a production line. The PMF of X is given by:\n",
    "\n",
    "P(X=0) = 0.6\n",
    "P(X=1) = 0.3\n",
    "P(X=2) = 0.1\n",
    "\n",
    "+ The CDF of X can be calculated by adding up the probabilities of X taking on values less than or equal to a given value.\n",
    "\n",
    "+ For example, the CDF at x=1 is:\n",
    "\n",
    "F(1) = P(X<=1) = P(X=0) + P(X=1) = 0.6 + 0.3 = 0.9\n",
    "\n",
    "+ The CDF tells us the probability that the random variable X takes on a value less than or equal to a given value. In other words, it gives us information about the distribution of the random variable. It is often used in statistical analysis to compare different distributions, to calculate percentiles, and to compute various statistical measures such as mean, variance, and standard deviation. Additionally, CDF is used in hypothesis testing, where we test whether a sample comes from a particular distribution or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af97f3-1180-44ce-8e25-6f43add67fda",
   "metadata": {},
   "source": [
    "# Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "# Explain how the parameters of the normal distribution relate to the shape of the distribution.\n",
    "\n",
    "## The normal distribution, also known as the Gaussian distribution, is a probability distribution that is widely used to model a wide range of natural and social phenomena. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1. Height: The distribution of heights in a population can be modeled using a normal distribution.\n",
    "\n",
    "2. IQ scores: IQ scores of individuals in a population can be modeled using a normal distribution.\n",
    "\n",
    "3. Errors in measurements: The errors in measurements of physical quantities, such as length or weight, can be modeled using a normal distribution.\n",
    "\n",
    "4. Stock prices: The daily returns of stock prices can be modeled using a normal distribution.\n",
    "\n",
    "5. Exam scores: The distribution of exam scores in a large class can often be approximated by a normal distribution.\n",
    "\n",
    "+ The normal distribution is defined by two parameters: the mean and the standard deviation. The mean represents the center of the distribution and the standard deviation represents the spread of the distribution.\n",
    "\n",
    "+ If the mean is large, the distribution will be shifted to the right, and if the mean is small, the distribution will be shifted to the left. If the standard deviation is small, the distribution will be narrow, and if the standard deviation is large, the distribution will be wide.\n",
    "\n",
    "+ The shape of the normal distribution is symmetrical around the mean, which means that the probability of getting a value below the mean is the same as the probability of getting a value above the mean. The peak of the distribution occurs at the mean, and the probability of getting a value far from the mean is very small. The normal distribution is a continuous distribution, which means that the probability of getting a particular value is zero, and the probability is calculated for a range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c5a56-c959-46e2-bafe-fc59962e2d5b",
   "metadata": {},
   "source": [
    "# Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution.\n",
    "\n",
    "## The normal distribution is an important concept in statistics and probability theory. It has many useful properties that make it a useful tool for modeling real-world phenomena. Here are some reasons why the normal distribution is important:\n",
    "\n",
    "1. Central Limit Theorem: The normal distribution is closely related to the central limit theorem, which states that the sum or average of a large number of independent, identically distributed random variables tends to follow a normal distribution. This makes the normal distribution a good model for many real-world phenomena.\n",
    "\n",
    "2. Probability Calculations: The normal distribution has a well-defined probability density function, which allows for easy calculation of probabilities and the determination of confidence intervals and hypothesis tests.\n",
    "\n",
    "3. Parameter Estimation: The normal distribution is often used in maximum likelihood estimation, which is a method for estimating the parameters of a statistical model based on a sample of data.\n",
    "\n",
    "4. Standardization: The normal distribution has a standard form with a mean of zero and a standard deviation of one, which makes it easy to compare and analyze different datasets.\n",
    "\n",
    "## Real-life examples of normal distribution include:\n",
    "\n",
    "1. IQ Scores: IQ scores are known to follow a normal distribution with a mean of 100 and a standard deviation of 15.\n",
    "\n",
    "2. Height: Height of people in a population is also known to follow a normal distribution, with the mean height being different for men and women, and with a standard deviation of around 3 inches.\n",
    "\n",
    "3. Exam Scores: Exam scores in large classes or standardized tests are often assumed to follow a normal distribution, with a mean around the passing grade and a standard deviation that depends on the difficulty of the exam.\n",
    "\n",
    "4. Blood Pressure: Blood pressure readings for a large population can also be modeled using a normal distribution, with a mean of around 120/80 mmHg and a standard deviation of around 10 mmHg.\n",
    "\n",
    "5. Financial Returns: The daily returns of many financial assets, such as stocks and currencies, are often modeled using a normal distribution, which is a key assumption in many financial models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36112bb8-20b7-4c46-a873-953d13d10fa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?\n",
    "\n",
    "+ The Bernoulli distribution is a discrete probability distribution that describes the outcomes of a single binary (yes/no) experiment, where the probability of success is p and the probability of failure is (1-p). It is named after the Swiss mathematician Jacob Bernoulli.\n",
    "\n",
    "+ For example, let's consider a coin flip experiment, where we define success as getting heads and failure as getting tails. In this case, the Bernoulli distribution can be used to model the probability of getting heads, given a certain probability of success.\n",
    "\n",
    "+ The probability mass function of the Bernoulli distribution is:\n",
    "\n",
    "P(X = x) = p^x * (1-p)^(1-x)\n",
    "\n",
    "+ where X is the random variable representing the outcome of the experiment, and x is either 0 (failure) or 1 (success).\n",
    "\n",
    "+ The main difference between the Bernoulli distribution and the binomial distribution is that the Bernoulli distribution models the outcome of a single binary experiment, while the binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "+ In other words, the binomial distribution is the sum of n independent Bernoulli trials, where n is a fixed number. The probability mass function of the binomial distribution is:\n",
    "\n",
    "P(X = k) = (n choose k) * p^k * (1-p)^(n-k)\n",
    "\n",
    "+ where X is the random variable representing the number of successes in n Bernoulli trials, k is the number of successes, p is the probability of success in each trial, and (n choose k) is the binomial coefficient.\n",
    "\n",
    "+ For example, let's consider a situation where we want to model the probability of getting exactly 3 heads in 5 coin flips. This can be modeled using a binomial distribution, where n=5 and p=0.5 (assuming the coin is fair). The probability of getting exactly 3 heads is:\n",
    "\n",
    "P(X = 3) = (5 choose 3) * 0.5^3 * 0.5^2 = 0.3125\n",
    "\n",
    "+ In summary, the Bernoulli distribution models the outcome of a single binary experiment, while the binomial distribution models the number of successes in a fixed number of independent Bernoulli trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57af27-f1c4-4413-ba2d-1a12a7ec5fce",
   "metadata": {},
   "source": [
    "# Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "# is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "# than 60? Use the appropriate formula and show your calculations.\n",
    "\n",
    "+ We can use the standard normal distribution to answer this question, since the dataset is assumed to be normally distributed. We first need to standardize the value of 60 using the formula:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "+ where x is the value we want to standardize, μ is the mean of the dataset, σ is the standard deviation of the dataset, and z is the standardized value in terms of the standard normal distribution.\n",
    "\n",
    "+ Plugging in the values, we get:\n",
    "\n",
    "z = (60 - 50) / 10 = 1\n",
    "\n",
    "+ This means that 60 is 1 standard deviation above the mean of the dataset.\n",
    "\n",
    "+ Next, we can use a standard normal distribution table or a calculator to find the probability that a randomly selected observation from a standard normal distribution is greater than 1. From the table or calculator, we find that this probability is approximately 0.1587.\n",
    "\n",
    "+ Therefore, the probability that a randomly selected observation from the given dataset will be greater than 60 is approximately 0.1587, assuming the dataset is normally distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86707f64-fd8a-428f-8bd9-55d88f7c7983",
   "metadata": {},
   "source": [
    "# Q7: Explain uniform Distribution with an example.\n",
    "\n",
    "+ The uniform distribution is a continuous probability distribution that describes a random variable that can take on any value within a specific range with equal probability. It is also known as the rectangular distribution.\n",
    "\n",
    "+ For example, let's consider the case of rolling a fair six-sided die. Each outcome (1, 2, 3, 4, 5, or 6) has an equal probability of occurring, which can be modeled using a uniform distribution. The probability density function of the uniform distribution is:\n",
    "\n",
    "f(x) = 1 / (b-a)\n",
    "\n",
    "+ where x is a random variable within the range [a, b], and f(x) is the probability density function.\n",
    "\n",
    "+ In the case of rolling a fair six-sided die, a = 1 and b = 6. Therefore, the probability density function is:\n",
    "\n",
    "f(x) = 1 / (6-1) = 1/5\n",
    "\n",
    "+ This means that the probability of rolling any number between 1 and 6 is 1/5, since each outcome is equally likely.\n",
    "\n",
    "+ The cumulative distribution function of the uniform distribution is:\n",
    "\n",
    "F(x) = (x-a) / (b-a) for a <= x < b\n",
    "F(x) = 0 for x < a\n",
    "F(x) = 1 for x >= b\n",
    "\n",
    "+ This function gives the probability that the random variable is less than or equal to a given value x.\n",
    "\n",
    "+ In summary, the uniform distribution is a probability distribution that models a random variable with equal probability of taking on any value within a specific range. It is often used in modeling scenarios where outcomes are equally likely, such as rolling dice or selecting a random number from a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b0b57-9d7c-4bcf-a326-b277a16bec25",
   "metadata": {},
   "source": [
    "# Q8: What is the z score? State the importance of the z score.\n",
    "\n",
    "+ The z-score, also known as the standard score, is a standardized value that indicates how many standard deviations a data point is away from the mean of a distribution. It is calculated as:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "+ where x is the data point, μ is the mean of the distribution, and σ is the standard deviation of the distribution.\n",
    "\n",
    "+ The z-score is important because it allows us to compare values from different distributions on a common scale. By standardizing values to z-scores, we can determine the relative position of a value within its distribution and compare it to the position of values in other distributions.\n",
    "\n",
    "+ For example, if we have two datasets with different means and standard deviations, we cannot compare values between the two datasets directly. However, if we convert each value to a z-score, we can compare the relative position of each value within its respective distribution, regardless of the original units of measurement or the characteristics of the distributions.\n",
    "\n",
    "+ The z-score is also important in hypothesis testing and statistical inference, as it allows us to calculate p-values and determine the statistical significance of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46484c97-a5bf-4d00-bdda-8d5aabe6d9d0",
   "metadata": {},
   "source": [
    "# Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.\n",
    "\n",
    "+ The Central Limit Theorem (CLT) is a fundamental theorem in statistics that describes the behavior of sample means when we draw repeated random samples from any population, regardless of its underlying distribution. It states that the distribution of sample means will be approximately normal, with a mean equal to the population mean, and a standard deviation equal to the population standard deviation divided by the square root of the sample size, as the sample size increases.\n",
    "\n",
    "+ In other words, the CLT states that as the sample size increases, the distribution of sample means becomes more and more normal, regardless of the underlying population distribution. This means that if we take multiple random samples from a population, the mean of the sample means will be close to the population mean, and the spread of the sample means will decrease as the sample size increases.\n",
    "\n",
    "+ The significance of the Central Limit Theorem is that it allows us to use statistical inference techniques, such as hypothesis testing and confidence intervals, on sample means, even if we do not know the underlying distribution of the population. This is because the CLT allows us to assume that the sample means are approximately normally distributed, which enables us to use the properties of the normal distribution to make inferences about the population.\n",
    "\n",
    "+ The Central Limit Theorem is used extensively in statistical analysis and is a key concept in many areas of science and research. It provides a foundation for the theory of statistical inference, and is essential for understanding and applying statistical methods in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a114b1d-d36e-42aa-b956-a952953c4d15",
   "metadata": {},
   "source": [
    "# Q10: State the assumptions of the Central Limit Theorem.\n",
    "\n",
    "## The Central Limit Theorem (CLT) is a fundamental theorem in statistics that applies to the behavior of sample means from any population, regardless of its underlying distribution, under certain assumptions. The assumptions of the CLT are:\n",
    "\n",
    "1. The sample size is sufficiently large: In general, the sample size should be greater than or equal to 30. However, the exact sample size required for the CLT to hold depends on the population distribution and the degree of skewness.\n",
    "\n",
    "2. The samples are drawn randomly and independently: Each sample must be selected independently of the other samples, and each observation within a sample must be independent of all other observations within that sample.\n",
    "\n",
    "3. The population distribution is not too skewed: The CLT assumes that the population distribution has a finite mean and a finite variance. If the population distribution is highly skewed or has heavy tails, then a large sample size may be required for the CLT to hold.\n",
    "\n",
    "4. The population size is much larger than the sample size: If the population size is close to the sample size, then a correction factor may need to be applied to the standard deviation of the sample mean to account for the finite population size.\n",
    "\n",
    "5. The measurements are additive: The CLT assumes that the observations are additive, meaning that the sum of the observations is meaningful and makes sense.\n",
    "\n",
    "+ These assumptions are critical for the validity of the CLT, and violations of these assumptions can lead to biased or unreliable results. Therefore, it is important to check whether these assumptions hold before applying the CLT to a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51ee03-be1a-42c6-b805-700a98d5a88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
