{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ff1ea1-76d8-427a-bf13-42912bde8958",
   "metadata": {},
   "source": [
    "# Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and\n",
    "# categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are\n",
    "# m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
    "# eng#neer#ng process and handles the m#ss#ng valuesD\n",
    "\n",
    "# a pipeline in Python that automates the feature engineering process and handles missing values\n",
    "\n",
    "1. Load the dataset and split it into training and testing sets.\n",
    "2. Identify the numerical and categorical features in the dataset.\n",
    "3. Handle missing values in the numerical features using mean imputation and in the categorical features using mode imputation.\n",
    "4. Use an automated feature selection method, such as SelectKBest, to select the most important features in the dataset.\n",
    "5. Scale the numerical features using StandardScaler to standardize the feature values.\n",
    "6. One-hot encode the categorical features using OneHotEncoder to convert categorical variables into binary variables.\n",
    "7. Merge the numerical and categorical features into a single feature matrix.\n",
    "8. Train a machine learning model on the feature matrix and evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3af57-b2ea-4bdd-bfe9-4985bdf79b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to implement this pipeline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical and categorical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "cat_features = [i for i in range(X.shape[1]) if i not in num_features]\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    # Handle missing values in numerical features using mean imputation\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    # Handle missing values in categorical features using mode imputation\n",
    "    ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    # Use SelectKBest to select the most important features\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
    "    # Scale the numerical features using StandardScaler\n",
    "    ('scaler', StandardScaler()),\n",
    "    # One-hot encode the categorical features using OneHotEncoder\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_transformed, y_test))\n",
    "\n",
    "# This pipeline can be modified based on the specific needs of our machine learning project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf95afd-cbd5-4e0a-8633-ed43a39f3621",
   "metadata": {},
   "source": [
    "# Create a numerical pipeline that includes the following steps\"\n",
    "\n",
    "1. Load the dataset and split it into training and testing sets.\n",
    "2. Identify the numerical features in the dataset.\n",
    "3. Handle missing values in the numerical features using mean imputation.\n",
    "4. Use an automated feature selection method, such as SelectKBest, to select the most important features in the dataset.\n",
    "5. Scale the numerical features using StandardScaler to standardize the feature values.\n",
    "6. Train a machine learning model on the selected features and evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e827e5-8faf-461f-815c-cdb57095846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to implement this numerical pipeline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "\n",
    "# Define the numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    # Handle missing values in numerical features using mean imputation\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    # Use SelectKBest to select the most important features\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
    "    # Scale the numerical features using StandardScaler\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Fit the numerical pipeline on the training data\n",
    "X_train_num = X_train[:, num_features]\n",
    "num_pipeline.fit(X_train_num, y_train)\n",
    "\n",
    "# Transform the numerical data using the fitted pipeline\n",
    "X_train_num_transformed = num_pipeline.transform(X_train_num)\n",
    "X_test_num_transformed = num_pipeline.transform(X_test[:, num_features])\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_num_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_num_transformed, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165c1af-709f-49ed-9f69-4449afd63ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values in the numerical columns using the mean of the column values\n",
    "\n",
    "# pipeline that includes the steps to handle missing values in numerical columns using the mean of the column values:,\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical and categorical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "cat_features = [i for i in range(X.shape[1]) if i not in num_features]\n",
    "\n",
    "# Define the numerical and categorical pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_transformed, y_test))\n",
    "\n",
    "\n",
    "\n",
    "In this pipeline, we use the SimpleImputer class to impute the missing values in the numerical columns using the mean of the column values. We define separate pipelines for the numerical and categorical features and combine them using ColumnTransformer. The pipeline handles missing values and scales the numerical features using StandardScaler.\n",
    "\n",
    "Note that we also handle missing values in the categorical features using the most frequent value imputation and use OneHotEncoder to convert the categorical features into numerical features.\n",
    "\n",
    "You can modify this pipeline based on your specific needs by adding or removing preprocessing steps or using different imputation or scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cff6b4-0ae1-4ce0-a016-8682593e69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical columns using standardisations\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical and categorical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "cat_features = [i for i in range(X.shape[1]) if i not in num_features]\n",
    "\n",
    "# Define the numerical and categorical pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_transformed, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7a33d-d619-4b3d-89b5-12c6a14c6571",
   "metadata": {},
   "source": [
    "+ In this modified pipeline, we use the StandardScaler class to scale the numerical columns using standardisation after imputing the missing values using the mean of the column values. The rest of the pipeline remains the same as the previous example.\n",
    "\n",
    "+ Note that standardisation scales the data to have zero mean and unit variance, which can help the model converge faster and improve its performance. You can modify this pipeline based on your specific needs by adding or removing preprocessing steps or using different scaling or imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bd4b6-b9dd-4cf6-81c1-0205778a01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values in the categorical columns using the most frequent value of the columns\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical and categorical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "cat_features = [i for i in range(X.shape[1]) if i not in num_features]\n",
    "\n",
    "# Define the numerical and categorical pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_transformed, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d48280-06af-4091-896e-f3c74a3bdf29",
   "metadata": {},
   "source": [
    "+ In this modified pipeline, we use the SimpleImputer class with 'strategy='most_frequent' to impute the missing values in the categorical columns using the most frequent value of the column. The rest of the pipeline remains the same as the previous example.\n",
    "\n",
    "+ Note that the choice of imputation strategy may depend on the nature of the missing values and the distribution of the data. You can modify this pipeline based on your specific needs by adding or removing preprocessing steps or using different imputation or encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b5e4f-4471-45fd-8641-039ce7d59309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the categorical columns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical and categorical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "cat_features = [i for i in range(X.shape[1]) if i not in num_features]\n",
    "\n",
    "# Define the numerical and categorical pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_transformed, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36fbb8-be74-472a-ba00-560c306f7cd6",
   "metadata": {},
   "source": [
    "+ In this modified pipeline, we use the OneHotEncoder class to one-hot encode the categorical columns. The 'handle_unknown='ignore' parameter is used to handle any unknown categories in the test data by ignoring them during encoding. The rest of the pipeline remains the same as the previous examples.\n",
    "\n",
    "+ Note that the choice of encoding method may depend on the nature of the categorical data and the specific requirements of your machine learning model. You can modify this pipeline based on your specific needs by adding or removing preprocessing steps or using different encoding or imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232b10d-a6cc-4a37-aa9d-cbdd01c0f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numerical and categorical pipelines using a ColumnTransformers\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify the numerical and categorical features\n",
    "num_features = [i for i in range(X.shape[1]) if data.feature_names[i].startswith(('mean', 'area', 'worst'))]\n",
    "cat_features = [i for i in range(X.shape[1]) if i not in num_features]\n",
    "\n",
    "# Define the numerical and categorical pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the transformed data and evaluate its performance\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('Accuracy on the testing set:', model.score(X_test_transformed, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdaae9-08cd-45a0-9342-0342403ad5a3",
   "metadata": {},
   "source": [
    "+ In this updated pipeline, we define two separate pipelines: one for the numerical features and one for the categorical features. We then use a 'ColumnTransformer' to combine these two pipelines and apply them to the appropriate columns of the input data.\n",
    "\n",
    "+ The 'ColumnTransformer' is defined with a list of tuples, where each tuple corresponds to a pipeline for a specific type of column (in this case, numerical or categorical). Each tuple includes the following three elements:\n",
    "\n",
    "1. A string that identifies the pipeline (in this case, 'num' or 'cat').\n",
    "2. The pipeline object for that type of column (in this case, 'num_pipeline' or 'cat_pipeline').\n",
    "3. The indices of the columns to which the pipeline should be applied (in this case, 'num_features' or 'cat_features').\n",
    "\n",
    "\n",
    "+ Once the 'ColumnTransformer' is defined, we fit it to the training data and use it to transform both the training and testing data. Finally, we train a logistic regression model on the transformed data and evaluate its performance.\n",
    "\n",
    "+ This pipeline automates the feature engineering process by imputing missing values, scaling numerical features, one-hot encoding categorical features, and combining the transformed features into a single dataset. You can modify this pipeline to suit your specific needs by adding or removing preprocessing steps or using different encoding or imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a5fcb-5558-4954-b0d5-2d4af1a53595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Random Forest Classifier to build the final models\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# separate the target variable from the features\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable']\n",
    "\n",
    "# divide the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the numerical pipeline\n",
    "num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# define the categorical pipeline\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "# combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, ['numerical_feature_1', 'numerical_feature_2', ...]),\n",
    "        ('cat', cat_pipeline, ['categorical_feature_1', 'categorical_feature_2', ...])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define the final Random Forest Classifier model\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Model Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac196c00-5fa3-4ee2-8ab6-ff71e8ef3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the model on the test datasets\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assume X and y are your feature and target variables\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the numerical and categorical pipelines\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# combine the pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# define the final model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test data and evaluate the model accuracy\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a693c-ab97-462b-9eb8-19916c6a887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Bu#ld a p#pel#ne that #ncludes a random forest class#f#er and a log#st#c regress#on class#f#er, and then\n",
    "# use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipelines for the Random Forest Classifier and the Logistic Regression Classifier\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Combine the pipelines using a Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_pipeline), ('lr', lr_pipeline)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Train the pipeline on the training dataset\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the pipeline on the test dataset\n",
    "accuracy = voting_clf.score(X_test, y_test)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
