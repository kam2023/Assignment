{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2324a-5053-44ff-90b9-ccbe685bcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions:\n",
    "\n",
    "import requests\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "# Destination file path to save the downloaded dataset\n",
    "destination_file = \"wine.data\"\n",
    "\n",
    "# Send a GET request to download the dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Save the downloaded content to a file\n",
    "    with open(destination_file, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933795f3-0129-4123-b6b5-7431190bd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas dataframe.\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66021878-71d6-4a4e-91ec-c5101b268089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into features and target variables.\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Display the shape of the feature matrix and target variable\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target variable shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf365c-48d9-49f3-80d4-e4b1ed10b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing (e.g., scaling, normalisation, missing value imputation) as necessary.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Imputer\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Perform data preprocessing steps\n",
    "\n",
    "# Scaling the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Normalizing the features using MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "\n",
    "# Imputing missing values using mean imputation\n",
    "imputer = Imputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(\"Scaled data:\\n\", X_scaled)\n",
    "print(\"Normalized data:\\n\", X_normalized)\n",
    "print(\"Imputed data:\\n\", X_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87a075-6459-45fb-b129-394bbb8e56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement PCA on the preprocessed dataset using the scikit-learn library.\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Preprocess the features by scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Specify the number of principal components to keep\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Display the explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Display the transformed data after PCA\n",
    "print(\"Transformed data after PCA:\\n\", X_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824b7f6-1449-4dd3-abdf-8a1146465b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of principal components to retain based on the explained variance ratio.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Preprocess the features by scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.plot(range(1, len(cumulative_var_ratio) + 1), cumulative_var_ratio)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a1daa-ca51-4541-b935-08b1bbaa23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results of PCA using a scatter plot.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Preprocess the features by scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Specify the number of principal components to retain\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a scatter plot of the transformed data\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab9328-d292-4aac-9e69-77f52c4fa9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on the PCA-transformed data using K-Means clustering algorithm.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Preprocess the features by scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Specify the number of principal components to retain\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3)  # Specify the number of clusters\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Create a scatter plot of the PCA-transformed data with cluster labels\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-Means Clustering on PCA-Transformed Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70c96a-e06f-41cb-a07a-1a2d8462014c",
   "metadata": {},
   "source": [
    "# Interpret the results of PCA and clustering analysis.\n",
    "\n",
    "+ Interpreting the results of PCA and clustering analysis involves understanding the information captured by PCA and the grouping of data points into clusters\n",
    "\n",
    "## PCA Results:\n",
    "\n",
    "+ Principal Component 1: This component represents the direction in the data with the highest variance. It captures the most significant patterns or trends in the original features. The x-axis of the scatter plot represents the variation along this component.\n",
    "+ Principal Component 2: This component represents the direction in the data orthogonal to Principal Component 1 with the second highest variance. It captures additional patterns or trends in the original features. The y-axis of the scatter plot represents the variation along this component.\n",
    "+ Explained Variance Ratio: The explained variance ratio indicates the proportion of variance in the original features explained by each principal component. It helps assess how much information is retained by the chosen number of components. The cumulative explained variance ratio curve shows the cumulative amount of variance explained as the number of components increases. In the example code, it is plotted to help determine the optimal number of components to retain.\n",
    "\n",
    "## Clustering Analysis:\n",
    "\n",
    "+ K-Means Clustering: The K-Means algorithm is applied to the PCA-transformed data to group similar data points into clusters based on their distances in the reduced feature space. In the scatter plot, the data points are color-coded according to the assigned clusters.\n",
    "\n",
    "## Interpretation:\n",
    "###  By combining PCA and clustering analysis, we can gain insights into the structure of the dataset. Here are some possible interpretations:\n",
    "\n",
    "+ Separation of Clusters: If the clusters in the scatter plot appear well-separated and distinct, it indicates that the K-Means algorithm successfully identified groups of similar data points based on the PCA-transformed features. The distance between the clusters implies that the identified groups have different characteristics or patterns.\n",
    "\n",
    "+ Overlapping Clusters: If the clusters overlap or have a less clear separation, it suggests that the data points might have some inherent complexity or noise, making it challenging to distinctly group them. In such cases, alternative clustering algorithms or feature engineering approaches might be explored.\n",
    "\n",
    "+ Relationship with Principal Components: By examining the scatter plot in conjunction with the PCA results, you can analyze how the clusters are related to the principal components. It helps understand which combinations of features contribute most to the cluster separation or overlap.\n",
    "\n",
    "+ Interpretation of Clusters: Based on domain knowledge or further analysis, you can interpret the clusters in terms of their characteristics or behaviors. For example, in the wine dataset, if the clusters correspond to different types of wine, you can assign labels to the clusters and infer that the algorithm successfully identified distinct wine types based on the PCA-transformed features.\n",
    "\n",
    "### It's important to note that the interpretation may vary depending on the specific dataset, the number of principal components retained, the number of clusters, and the domain knowledge. Therefore, it's crucial to analyze and interpret the results in the context of the specific problem or analysis being conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1b444-c5ed-44ed-8501-57cf5c8ae460",
   "metadata": {},
   "source": [
    "# Deliverables:\n",
    "\n",
    "## PCA and Clustering Analysis Report\n",
    "\n",
    "### In this report, we summarize the results of Principal Component Analysis (PCA) and clustering analysis on the wine dataset.\n",
    "\n",
    "###  1. Principal Component Analysis (PCA) Results:\n",
    "\n",
    "###  PCA was performed on the wine dataset to reduce the dimensionality of the data and capture its inherent patterns and trends. The following key results were obtained:\n",
    "\n",
    "+ Principal Component 1: This component represents the direction in the data with the highest variance. It captures the most significant patterns or trends in the original features.\n",
    "\n",
    "+ Principal Component 2: This component represents the direction in the data orthogonal to Principal Component 1 with the second highest variance. It captures additional patterns or trends in the original features.\n",
    "\n",
    "+ Explained Variance Ratio: The explained variance ratio indicates the proportion of variance in the original features explained by each principal component. It helps assess how much information is retained by the chosen number of components. The cumulative explained variance ratio curve showed the cumulative amount of variance explained as the number of components increased.\n",
    "\n",
    "## 2. Clustering Analysis Results:\n",
    "\n",
    "K-Means clustering was applied to the PCA-transformed data to group similar data points into clusters based on their distances in the reduced feature space. The following results were observed:\n",
    "\n",
    "+ Cluster Separation: The scatter plot of the PCA-transformed data with cluster labels showed the separation or overlap of clusters. If the clusters appeared well-separated, it indicated that the K-Means algorithm successfully identified groups of similar data points based on the PCA-transformed features. If the clusters overlapped, it suggested inherent complexity or noise in the data, making it challenging to distinctly group the points.\n",
    "\n",
    "+ Relationship with Principal Components: The relationship between the clusters and the principal components was examined to understand which combinations of features contributed most to the cluster separation or overlap. The clustering results were analyzed in conjunction with the PCA results to gain insights into the structure of the dataset.\n",
    "\n",
    "## 3. Interpretation:\n",
    "\n",
    "Based on the results obtained, the following interpretations can be made:\n",
    "\n",
    "+ The PCA analysis revealed the directions in the data with the highest variances, which represent the most significant patterns or trends in the original features.\n",
    "\n",
    "+ The clustering analysis, based on the PCA-transformed data, successfully grouped similar data points into clusters. The scatter plot visually depicted the separation or overlap of clusters.\n",
    "\n",
    "+ The interpretation of the clusters could be performed based on domain knowledge or further analysis. For example, in the wine dataset, if the clusters correspond to different types of wine, it can be inferred that the algorithm successfully identified distinct wine types based on the PCA-transformed features.\n",
    "\n",
    "## 4. Recommendations and Further Analysis:\n",
    "\n",
    "+ The optimal number of principal components to retain can be determined based on the explained variance ratio. It is important to strike a balance between reducing dimensionality and retaining sufficient information. The cumulative explained variance ratio curve can help in making an informed decision about the number of components to retain.\n",
    "\n",
    "+ Additional clustering algorithms or feature engineering approaches can be explored to validate and compare the results obtained from K-Means clustering. Different algorithms may provide different perspectives on the grouping of data points.\n",
    "\n",
    "+ The interpretation of the clusters should be further analyzed and validated with domain knowledge or additional statistical techniques. This will help in gaining a deeper understanding of the underlying patterns and characteristics of the data.\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "+ PCA and clustering analysis provided insights into the wine dataset by reducing its dimensionality and identifying groups of similar data points. The results of PCA helped understand the significant patterns and trends in the data, while the clustering analysis allowed for the identification of distinct clusters. These findings contribute to a better understanding of the dataset and can guide further analysis or decision-making processes.\n",
    "\n",
    "+ It is important to note that the interpretation and conclusions drawn from the analysis are specific to the wine dataset used in this report. The findings may vary for different datasets and should be interpreted within the context of the specific problem or analysis being conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fedcb88-a814-474b-8e09-79e1bccdbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot showing the results of PCA.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Preprocess the features by scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Specify the number of principal components to retain\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a scatter plot of the transformed data\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fbbe4-a09f-4029-b36a-0b0259bab588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A table showing the performance metrics for the clustering algorithm.\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Path to the downloaded dataset\n",
    "dataset_file = \"wine.data\"\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv(dataset_file, header=None)\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3)  # Specify the number of clusters\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Calculate performance metrics\n",
    "silhouette = silhouette_score(X, cluster_labels)\n",
    "ari = adjusted_rand_score(y, cluster_labels)\n",
    "\n",
    "# Create a table to display the performance metrics\n",
    "metrics_table = pd.DataFrame({'Silhouette Score': [silhouette], 'Adjusted Rand Index (ARI)': [ari]})\n",
    "print(metrics_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
